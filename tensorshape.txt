model.embed_tokens.weight [151936, 896] [0, 272269312]
model.layers.0.input_layernorm.weight [896] [272269312, 272271104]
model.layers.0.mlp.down_proj.weight [896, 4864] [272271104, 280987392]
model.layers.0.mlp.gate_proj.weight [4864, 896] [280987392, 289703680]
model.layers.0.mlp.up_proj.weight [4864, 896] [289703680, 298419968]
model.layers.0.post_attention_layernorm.weight [896] [298419968, 298421760]
model.layers.0.self_attn.k_proj.bias [128] [298421760, 298422016]
model.layers.0.self_attn.k_proj.weight [128, 896] [298422016, 298651392]
model.layers.0.self_attn.o_proj.weight [896, 896] [298651392, 300257024]
model.layers.0.self_attn.q_proj.bias [896] [300257024, 300258816]
model.layers.0.self_attn.q_proj.weight [896, 896] [300258816, 301864448]
model.layers.0.self_attn.v_proj.bias [128] [301864448, 301864704]
model.layers.0.self_attn.v_proj.weight [128, 896] [301864704, 302094080]
model.layers.1.input_layernorm.weight [896] [302094080, 302095872]
model.layers.1.mlp.down_proj.weight [896, 4864] [302095872, 310812160]
model.layers.1.mlp.gate_proj.weight [4864, 896] [310812160, 319528448]
model.layers.1.mlp.up_proj.weight [4864, 896] [319528448, 328244736]
model.layers.1.post_attention_layernorm.weight [896] [328244736, 328246528]
model.layers.1.self_attn.k_proj.bias [128] [328246528, 328246784]
model.layers.1.self_attn.k_proj.weight [128, 896] [328246784, 328476160]
model.layers.1.self_attn.o_proj.weight [896, 896] [328476160, 330081792]
model.layers.1.self_attn.q_proj.bias [896] [330081792, 330083584]
model.layers.1.self_attn.q_proj.weight [896, 896] [330083584, 331689216]
model.layers.1.self_attn.v_proj.bias [128] [331689216, 331689472]
model.layers.1.self_attn.v_proj.weight [128, 896] [331689472, 331918848]
model.layers.10.input_layernorm.weight [896] [331918848, 331920640]
model.layers.10.mlp.down_proj.weight [896, 4864] [331920640, 340636928]
model.layers.10.mlp.gate_proj.weight [4864, 896] [340636928, 349353216]
model.layers.10.mlp.up_proj.weight [4864, 896] [349353216, 358069504]
model.layers.10.post_attention_layernorm.weight [896] [358069504, 358071296]
model.layers.10.self_attn.k_proj.bias [128] [358071296, 358071552]
model.layers.10.self_attn.k_proj.weight [128, 896] [358071552, 358300928]
model.layers.10.self_attn.o_proj.weight [896, 896] [358300928, 359906560]
model.layers.10.self_attn.q_proj.bias [896] [359906560, 359908352]
model.layers.10.self_attn.q_proj.weight [896, 896] [359908352, 361513984]
model.layers.10.self_attn.v_proj.bias [128] [361513984, 361514240]
model.layers.10.self_attn.v_proj.weight [128, 896] [361514240, 361743616]
model.layers.11.input_layernorm.weight [896] [361743616, 361745408]
model.layers.11.mlp.down_proj.weight [896, 4864] [361745408, 370461696]
model.layers.11.mlp.gate_proj.weight [4864, 896] [370461696, 379177984]
model.layers.11.mlp.up_proj.weight [4864, 896] [379177984, 387894272]
model.layers.11.post_attention_layernorm.weight [896] [387894272, 387896064]
model.layers.11.self_attn.k_proj.bias [128] [387896064, 387896320]
model.layers.11.self_attn.k_proj.weight [128, 896] [387896320, 388125696]
model.layers.11.self_attn.o_proj.weight [896, 896] [388125696, 389731328]
model.layers.11.self_attn.q_proj.bias [896] [389731328, 389733120]
model.layers.11.self_attn.q_proj.weight [896, 896] [389733120, 391338752]
model.layers.11.self_attn.v_proj.bias [128] [391338752, 391339008]
model.layers.11.self_attn.v_proj.weight [128, 896] [391339008, 391568384]
model.layers.12.input_layernorm.weight [896] [391568384, 391570176]
model.layers.12.mlp.down_proj.weight [896, 4864] [391570176, 400286464]
model.layers.12.mlp.gate_proj.weight [4864, 896] [400286464, 409002752]
model.layers.12.mlp.up_proj.weight [4864, 896] [409002752, 417719040]
model.layers.12.post_attention_layernorm.weight [896] [417719040, 417720832]
model.layers.12.self_attn.k_proj.bias [128] [417720832, 417721088]
model.layers.12.self_attn.k_proj.weight [128, 896] [417721088, 417950464]
model.layers.12.self_attn.o_proj.weight [896, 896] [417950464, 419556096]
model.layers.12.self_attn.q_proj.bias [896] [419556096, 419557888]
model.layers.12.self_attn.q_proj.weight [896, 896] [419557888, 421163520]
model.layers.12.self_attn.v_proj.bias [128] [421163520, 421163776]
model.layers.12.self_attn.v_proj.weight [128, 896] [421163776, 421393152]
model.layers.13.input_layernorm.weight [896] [421393152, 421394944]
model.layers.13.mlp.down_proj.weight [896, 4864] [421394944, 430111232]
model.layers.13.mlp.gate_proj.weight [4864, 896] [430111232, 438827520]
model.layers.13.mlp.up_proj.weight [4864, 896] [438827520, 447543808]
model.layers.13.post_attention_layernorm.weight [896] [447543808, 447545600]
model.layers.13.self_attn.k_proj.bias [128] [447545600, 447545856]
model.layers.13.self_attn.k_proj.weight [128, 896] [447545856, 447775232]
model.layers.13.self_attn.o_proj.weight [896, 896] [447775232, 449380864]
model.layers.13.self_attn.q_proj.bias [896] [449380864, 449382656]
model.layers.13.self_attn.q_proj.weight [896, 896] [449382656, 450988288]
model.layers.13.self_attn.v_proj.bias [128] [450988288, 450988544]
model.layers.13.self_attn.v_proj.weight [128, 896] [450988544, 451217920]
model.layers.14.input_layernorm.weight [896] [451217920, 451219712]
model.layers.14.mlp.down_proj.weight [896, 4864] [451219712, 459936000]
model.layers.14.mlp.gate_proj.weight [4864, 896] [459936000, 468652288]
model.layers.14.mlp.up_proj.weight [4864, 896] [468652288, 477368576]
model.layers.14.post_attention_layernorm.weight [896] [477368576, 477370368]
model.layers.14.self_attn.k_proj.bias [128] [477370368, 477370624]
model.layers.14.self_attn.k_proj.weight [128, 896] [477370624, 477600000]
model.layers.14.self_attn.o_proj.weight [896, 896] [477600000, 479205632]
model.layers.14.self_attn.q_proj.bias [896] [479205632, 479207424]
model.layers.14.self_attn.q_proj.weight [896, 896] [479207424, 480813056]
model.layers.14.self_attn.v_proj.bias [128] [480813056, 480813312]
model.layers.14.self_attn.v_proj.weight [128, 896] [480813312, 481042688]
model.layers.15.input_layernorm.weight [896] [481042688, 481044480]
model.layers.15.mlp.down_proj.weight [896, 4864] [481044480, 489760768]
model.layers.15.mlp.gate_proj.weight [4864, 896] [489760768, 498477056]
model.layers.15.mlp.up_proj.weight [4864, 896] [498477056, 507193344]
model.layers.15.post_attention_layernorm.weight [896] [507193344, 507195136]
model.layers.15.self_attn.k_proj.bias [128] [507195136, 507195392]
model.layers.15.self_attn.k_proj.weight [128, 896] [507195392, 507424768]
model.layers.15.self_attn.o_proj.weight [896, 896] [507424768, 509030400]
model.layers.15.self_attn.q_proj.bias [896] [509030400, 509032192]
model.layers.15.self_attn.q_proj.weight [896, 896] [509032192, 510637824]
model.layers.15.self_attn.v_proj.bias [128] [510637824, 510638080]
model.layers.15.self_attn.v_proj.weight [128, 896] [510638080, 510867456]
model.layers.16.input_layernorm.weight [896] [510867456, 510869248]
model.layers.16.mlp.down_proj.weight [896, 4864] [510869248, 519585536]
model.layers.16.mlp.gate_proj.weight [4864, 896] [519585536, 528301824]
model.layers.16.mlp.up_proj.weight [4864, 896] [528301824, 537018112]
model.layers.16.post_attention_layernorm.weight [896] [537018112, 537019904]
model.layers.16.self_attn.k_proj.bias [128] [537019904, 537020160]
model.layers.16.self_attn.k_proj.weight [128, 896] [537020160, 537249536]
model.layers.16.self_attn.o_proj.weight [896, 896] [537249536, 538855168]
model.layers.16.self_attn.q_proj.bias [896] [538855168, 538856960]
model.layers.16.self_attn.q_proj.weight [896, 896] [538856960, 540462592]
model.layers.16.self_attn.v_proj.bias [128] [540462592, 540462848]
model.layers.16.self_attn.v_proj.weight [128, 896] [540462848, 540692224]
model.layers.17.input_layernorm.weight [896] [540692224, 540694016]
model.layers.17.mlp.down_proj.weight [896, 4864] [540694016, 549410304]
model.layers.17.mlp.gate_proj.weight [4864, 896] [549410304, 558126592]
model.layers.17.mlp.up_proj.weight [4864, 896] [558126592, 566842880]
model.layers.17.post_attention_layernorm.weight [896] [566842880, 566844672]
model.layers.17.self_attn.k_proj.bias [128] [566844672, 566844928]
model.layers.17.self_attn.k_proj.weight [128, 896] [566844928, 567074304]
model.layers.17.self_attn.o_proj.weight [896, 896] [567074304, 568679936]
model.layers.17.self_attn.q_proj.bias [896] [568679936, 568681728]
model.layers.17.self_attn.q_proj.weight [896, 896] [568681728, 570287360]
model.layers.17.self_attn.v_proj.bias [128] [570287360, 570287616]
model.layers.17.self_attn.v_proj.weight [128, 896] [570287616, 570516992]
model.layers.18.input_layernorm.weight [896] [570516992, 570518784]
model.layers.18.mlp.down_proj.weight [896, 4864] [570518784, 579235072]
model.layers.18.mlp.gate_proj.weight [4864, 896] [579235072, 587951360]
model.layers.18.mlp.up_proj.weight [4864, 896] [587951360, 596667648]
model.layers.18.post_attention_layernorm.weight [896] [596667648, 596669440]
model.layers.18.self_attn.k_proj.bias [128] [596669440, 596669696]
model.layers.18.self_attn.k_proj.weight [128, 896] [596669696, 596899072]
model.layers.18.self_attn.o_proj.weight [896, 896] [596899072, 598504704]
model.layers.18.self_attn.q_proj.bias [896] [598504704, 598506496]
model.layers.18.self_attn.q_proj.weight [896, 896] [598506496, 600112128]
model.layers.18.self_attn.v_proj.bias [128] [600112128, 600112384]
model.layers.18.self_attn.v_proj.weight [128, 896] [600112384, 600341760]
model.layers.19.input_layernorm.weight [896] [600341760, 600343552]
model.layers.19.mlp.down_proj.weight [896, 4864] [600343552, 609059840]
model.layers.19.mlp.gate_proj.weight [4864, 896] [609059840, 617776128]
model.layers.19.mlp.up_proj.weight [4864, 896] [617776128, 626492416]
model.layers.19.post_attention_layernorm.weight [896] [626492416, 626494208]
model.layers.19.self_attn.k_proj.bias [128] [626494208, 626494464]
model.layers.19.self_attn.k_proj.weight [128, 896] [626494464, 626723840]
model.layers.19.self_attn.o_proj.weight [896, 896] [626723840, 628329472]
model.layers.19.self_attn.q_proj.bias [896] [628329472, 628331264]
model.layers.19.self_attn.q_proj.weight [896, 896] [628331264, 629936896]
model.layers.19.self_attn.v_proj.bias [128] [629936896, 629937152]
model.layers.19.self_attn.v_proj.weight [128, 896] [629937152, 630166528]
model.layers.2.input_layernorm.weight [896] [630166528, 630168320]
model.layers.2.mlp.down_proj.weight [896, 4864] [630168320, 638884608]
model.layers.2.mlp.gate_proj.weight [4864, 896] [638884608, 647600896]
model.layers.2.mlp.up_proj.weight [4864, 896] [647600896, 656317184]
model.layers.2.post_attention_layernorm.weight [896] [656317184, 656318976]
model.layers.2.self_attn.k_proj.bias [128] [656318976, 656319232]
model.layers.2.self_attn.k_proj.weight [128, 896] [656319232, 656548608]
model.layers.2.self_attn.o_proj.weight [896, 896] [656548608, 658154240]
model.layers.2.self_attn.q_proj.bias [896] [658154240, 658156032]
model.layers.2.self_attn.q_proj.weight [896, 896] [658156032, 659761664]
model.layers.2.self_attn.v_proj.bias [128] [659761664, 659761920]
model.layers.2.self_attn.v_proj.weight [128, 896] [659761920, 659991296]
model.layers.20.input_layernorm.weight [896] [659991296, 659993088]
model.layers.20.mlp.down_proj.weight [896, 4864] [659993088, 668709376]
model.layers.20.mlp.gate_proj.weight [4864, 896] [668709376, 677425664]
model.layers.20.mlp.up_proj.weight [4864, 896] [677425664, 686141952]
model.layers.20.post_attention_layernorm.weight [896] [686141952, 686143744]
model.layers.20.self_attn.k_proj.bias [128] [686143744, 686144000]
model.layers.20.self_attn.k_proj.weight [128, 896] [686144000, 686373376]
model.layers.20.self_attn.o_proj.weight [896, 896] [686373376, 687979008]
model.layers.20.self_attn.q_proj.bias [896] [687979008, 687980800]
model.layers.20.self_attn.q_proj.weight [896, 896] [687980800, 689586432]
model.layers.20.self_attn.v_proj.bias [128] [689586432, 689586688]
model.layers.20.self_attn.v_proj.weight [128, 896] [689586688, 689816064]
model.layers.21.input_layernorm.weight [896] [689816064, 689817856]
model.layers.21.mlp.down_proj.weight [896, 4864] [689817856, 698534144]
model.layers.21.mlp.gate_proj.weight [4864, 896] [698534144, 707250432]
model.layers.21.mlp.up_proj.weight [4864, 896] [707250432, 715966720]
model.layers.21.post_attention_layernorm.weight [896] [715966720, 715968512]
model.layers.21.self_attn.k_proj.bias [128] [715968512, 715968768]
model.layers.21.self_attn.k_proj.weight [128, 896] [715968768, 716198144]
model.layers.21.self_attn.o_proj.weight [896, 896] [716198144, 717803776]
model.layers.21.self_attn.q_proj.bias [896] [717803776, 717805568]
model.layers.21.self_attn.q_proj.weight [896, 896] [717805568, 719411200]
model.layers.21.self_attn.v_proj.bias [128] [719411200, 719411456]
model.layers.21.self_attn.v_proj.weight [128, 896] [719411456, 719640832]
model.layers.22.input_layernorm.weight [896] [719640832, 719642624]
model.layers.22.mlp.down_proj.weight [896, 4864] [719642624, 728358912]
model.layers.22.mlp.gate_proj.weight [4864, 896] [728358912, 737075200]
model.layers.22.mlp.up_proj.weight [4864, 896] [737075200, 745791488]
model.layers.22.post_attention_layernorm.weight [896] [745791488, 745793280]
model.layers.22.self_attn.k_proj.bias [128] [745793280, 745793536]
model.layers.22.self_attn.k_proj.weight [128, 896] [745793536, 746022912]
model.layers.22.self_attn.o_proj.weight [896, 896] [746022912, 747628544]
model.layers.22.self_attn.q_proj.bias [896] [747628544, 747630336]
model.layers.22.self_attn.q_proj.weight [896, 896] [747630336, 749235968]
model.layers.22.self_attn.v_proj.bias [128] [749235968, 749236224]
model.layers.22.self_attn.v_proj.weight [128, 896] [749236224, 749465600]
model.layers.23.input_layernorm.weight [896] [749465600, 749467392]
model.layers.23.mlp.down_proj.weight [896, 4864] [749467392, 758183680]
model.layers.23.mlp.gate_proj.weight [4864, 896] [758183680, 766899968]
model.layers.23.mlp.up_proj.weight [4864, 896] [766899968, 775616256]
model.layers.23.post_attention_layernorm.weight [896] [775616256, 775618048]
model.layers.23.self_attn.k_proj.bias [128] [775618048, 775618304]
model.layers.23.self_attn.k_proj.weight [128, 896] [775618304, 775847680]
model.layers.23.self_attn.o_proj.weight [896, 896] [775847680, 777453312]
model.layers.23.self_attn.q_proj.bias [896] [777453312, 777455104]
model.layers.23.self_attn.q_proj.weight [896, 896] [777455104, 779060736]
model.layers.23.self_attn.v_proj.bias [128] [779060736, 779060992]
model.layers.23.self_attn.v_proj.weight [128, 896] [779060992, 779290368]
model.layers.3.input_layernorm.weight [896] [779290368, 779292160]
model.layers.3.mlp.down_proj.weight [896, 4864] [779292160, 788008448]
model.layers.3.mlp.gate_proj.weight [4864, 896] [788008448, 796724736]
model.layers.3.mlp.up_proj.weight [4864, 896] [796724736, 805441024]
model.layers.3.post_attention_layernorm.weight [896] [805441024, 805442816]
model.layers.3.self_attn.k_proj.bias [128] [805442816, 805443072]
model.layers.3.self_attn.k_proj.weight [128, 896] [805443072, 805672448]
model.layers.3.self_attn.o_proj.weight [896, 896] [805672448, 807278080]
model.layers.3.self_attn.q_proj.bias [896] [807278080, 807279872]
model.layers.3.self_attn.q_proj.weight [896, 896] [807279872, 808885504]
model.layers.3.self_attn.v_proj.bias [128] [808885504, 808885760]
model.layers.3.self_attn.v_proj.weight [128, 896] [808885760, 809115136]
model.layers.4.input_layernorm.weight [896] [809115136, 809116928]
model.layers.4.mlp.down_proj.weight [896, 4864] [809116928, 817833216]
model.layers.4.mlp.gate_proj.weight [4864, 896] [817833216, 826549504]
model.layers.4.mlp.up_proj.weight [4864, 896] [826549504, 835265792]
model.layers.4.post_attention_layernorm.weight [896] [835265792, 835267584]
model.layers.4.self_attn.k_proj.bias [128] [835267584, 835267840]
model.layers.4.self_attn.k_proj.weight [128, 896] [835267840, 835497216]
model.layers.4.self_attn.o_proj.weight [896, 896] [835497216, 837102848]
model.layers.4.self_attn.q_proj.bias [896] [837102848, 837104640]
model.layers.4.self_attn.q_proj.weight [896, 896] [837104640, 838710272]
model.layers.4.self_attn.v_proj.bias [128] [838710272, 838710528]
model.layers.4.self_attn.v_proj.weight [128, 896] [838710528, 838939904]
model.layers.5.input_layernorm.weight [896] [838939904, 838941696]
model.layers.5.mlp.down_proj.weight [896, 4864] [838941696, 847657984]
model.layers.5.mlp.gate_proj.weight [4864, 896] [847657984, 856374272]
model.layers.5.mlp.up_proj.weight [4864, 896] [856374272, 865090560]
model.layers.5.post_attention_layernorm.weight [896] [865090560, 865092352]
model.layers.5.self_attn.k_proj.bias [128] [865092352, 865092608]
model.layers.5.self_attn.k_proj.weight [128, 896] [865092608, 865321984]
model.layers.5.self_attn.o_proj.weight [896, 896] [865321984, 866927616]
model.layers.5.self_attn.q_proj.bias [896] [866927616, 866929408]
model.layers.5.self_attn.q_proj.weight [896, 896] [866929408, 868535040]
model.layers.5.self_attn.v_proj.bias [128] [868535040, 868535296]
model.layers.5.self_attn.v_proj.weight [128, 896] [868535296, 868764672]
model.layers.6.input_layernorm.weight [896] [868764672, 868766464]
model.layers.6.mlp.down_proj.weight [896, 4864] [868766464, 877482752]
model.layers.6.mlp.gate_proj.weight [4864, 896] [877482752, 886199040]
model.layers.6.mlp.up_proj.weight [4864, 896] [886199040, 894915328]
model.layers.6.post_attention_layernorm.weight [896] [894915328, 894917120]
model.layers.6.self_attn.k_proj.bias [128] [894917120, 894917376]
model.layers.6.self_attn.k_proj.weight [128, 896] [894917376, 895146752]
model.layers.6.self_attn.o_proj.weight [896, 896] [895146752, 896752384]
model.layers.6.self_attn.q_proj.bias [896] [896752384, 896754176]
model.layers.6.self_attn.q_proj.weight [896, 896] [896754176, 898359808]
model.layers.6.self_attn.v_proj.bias [128] [898359808, 898360064]
model.layers.6.self_attn.v_proj.weight [128, 896] [898360064, 898589440]
model.layers.7.input_layernorm.weight [896] [898589440, 898591232]
model.layers.7.mlp.down_proj.weight [896, 4864] [898591232, 907307520]
model.layers.7.mlp.gate_proj.weight [4864, 896] [907307520, 916023808]
model.layers.7.mlp.up_proj.weight [4864, 896] [916023808, 924740096]
model.layers.7.post_attention_layernorm.weight [896] [924740096, 924741888]
model.layers.7.self_attn.k_proj.bias [128] [924741888, 924742144]
model.layers.7.self_attn.k_proj.weight [128, 896] [924742144, 924971520]
model.layers.7.self_attn.o_proj.weight [896, 896] [924971520, 926577152]
model.layers.7.self_attn.q_proj.bias [896] [926577152, 926578944]
model.layers.7.self_attn.q_proj.weight [896, 896] [926578944, 928184576]
model.layers.7.self_attn.v_proj.bias [128] [928184576, 928184832]
model.layers.7.self_attn.v_proj.weight [128, 896] [928184832, 928414208]
model.layers.8.input_layernorm.weight [896] [928414208, 928416000]
model.layers.8.mlp.down_proj.weight [896, 4864] [928416000, 937132288]
model.layers.8.mlp.gate_proj.weight [4864, 896] [937132288, 945848576]
model.layers.8.mlp.up_proj.weight [4864, 896] [945848576, 954564864]
model.layers.8.post_attention_layernorm.weight [896] [954564864, 954566656]
model.layers.8.self_attn.k_proj.bias [128] [954566656, 954566912]
model.layers.8.self_attn.k_proj.weight [128, 896] [954566912, 954796288]
model.layers.8.self_attn.o_proj.weight [896, 896] [954796288, 956401920]
model.layers.8.self_attn.q_proj.bias [896] [956401920, 956403712]
model.layers.8.self_attn.q_proj.weight [896, 896] [956403712, 958009344]
model.layers.8.self_attn.v_proj.bias [128] [958009344, 958009600]
model.layers.8.self_attn.v_proj.weight [128, 896] [958009600, 958238976]
model.layers.9.input_layernorm.weight [896] [958238976, 958240768]
model.layers.9.mlp.down_proj.weight [896, 4864] [958240768, 966957056]
model.layers.9.mlp.gate_proj.weight [4864, 896] [966957056, 975673344]
model.layers.9.mlp.up_proj.weight [4864, 896] [975673344, 984389632]
model.layers.9.post_attention_layernorm.weight [896] [984389632, 984391424]
model.layers.9.self_attn.k_proj.bias [128] [984391424, 984391680]
model.layers.9.self_attn.k_proj.weight [128, 896] [984391680, 984621056]
model.layers.9.self_attn.o_proj.weight [896, 896] [984621056, 986226688]
model.layers.9.self_attn.q_proj.bias [896] [986226688, 986228480]
model.layers.9.self_attn.q_proj.weight [896, 896] [986228480, 987834112]
model.layers.9.self_attn.v_proj.bias [128] [987834112, 987834368]
model.layers.9.self_attn.v_proj.weight [128, 896] [987834368, 988063744]
model.norm.weight [896] [988063744, 988065536]
